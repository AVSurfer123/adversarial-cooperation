# --- DQN specific parameters ---

# use epsilon greedy action selector
action_selector: "epsilon_greedy"
epsilon_start: 1.0
epsilon_finish: 0.05
epsilon_anneal_time: 50000

# Not sure whether episode vs parallel matters, but parallel should be faster
# runner: "episode"
runner: "episode"
# batch_size_run: 8

buffer_size: 5000

mac: "def_gan_mac" # Adversarial controller

# use the Q_Learner to train
learner: "def_gan_learner"
gan_hidden_size: 100
gan_noise_size: 64
lr: .0008

# Args for loading the QMIX agent
agent_output_type: "q"
double_q: True
mixer: "qmix"
mixing_embed_dim: 32
target_update_interval: 200

name: "def_gan_testing"

test_interval: 10000
test_nepisode: 32


# CUSTOM:
save_model: True # Save the models to disk
save_model_interval: 500000 # Save models after this many timesteps

# Path to trained QMIX policy for N-1 agents
trained_agent_policy: "results/models/qmix_2s3z/2000079"
trained_adv_policy: "results/models/adv_dqn_parallel/2001012"
trained_obs_generator: "results/models/gen_obs/2001898"
checkpoint_path: "results/models/def_gan_working"

evaluate: True
